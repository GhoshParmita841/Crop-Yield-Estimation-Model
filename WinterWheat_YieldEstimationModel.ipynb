{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "@author: Parmita Ghosh\n",
    "\n",
    "Synergy of optical and synthetic aperture radar (SAR) data for early-stage crop yield estimation: A case study over a state of Germany\n",
    "Methodology:\n",
    "Step 1: Baseline Random Forest Regression Model\n",
    "In this notebook we are importing ground truth yield data set along with Optical and SAR image features. 70% of the data is being used for taring and 30% for testing the model. \n",
    "First a baseline random forest regression model with tuned hyper-paramets is being developed with all the input image features.\n",
    "\n",
    "Step2: The performance of baseline model is being evaluated by correlation coeffifient (r), Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) of obsered yield and predicted yield\n",
    "\n",
    "Step 3: An omtimised random forest regression with genetic algorithm (GA) based feature selection is being developed. \n",
    "This GA feature selection algorithm selected the best set of input image features for yied estimation with random forest regression model.\n",
    "\n",
    "Step 4: The performance of omtimised random forest regression with genetic algorithm (GA) based feature selection is being evaluated by correlation coeffifient (r), Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) of obsered yield and predicted yield\n",
    "\n",
    "Step 5: Visualisation of baseline and optimised model performance\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "######################################################################  Step 1: Baseline Random Forest Regression Model ####################################\n",
    "######################\n",
    "######################################################   Data Preparation ##############\n",
    "# Pandas is used for data manipulation\n",
    "import pandas as pd\n",
    "features=pd.read_csv('WinterWheat_SampleData.csv')#Reading the Winter Wheat Ground Truth data in csv Format  \n",
    "features.head(5) #Display first 5 rows\n",
    "\n",
    "\n",
    "# Use numpy to convert to arrays\n",
    "import numpy as np\n",
    "\n",
    "labels = np.array(features['Yield'])# Labels are the Crop Yield values\n",
    "features= features.drop('Yield', axis = 1)# Remove the Crop Yield from the features\n",
    "feature_list = list(features.columns)# Saving feature names for later use\n",
    "features = np.array(features)# Convert features to numpy array\n",
    "\n",
    "############################################## Tarin, Test data Preparation #########################\n",
    "\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.30, random_state = 42)# Split the data into 70% training and 30% testing sets\n",
    "\n",
    "###################################### Baseline Random Forest Model Development and Hyper-Parameters Tuning with GridSearchCV#######################\n",
    "\n",
    "#Importing required libraries\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rfr=RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [int(x) for x in np.linspace(10,100,10,endpoint=True,dtype = int)],\n",
    "    'min_samples_leaf': [int(x) for x in np.linspace(2,10,9,endpoint=True,dtype = int)],\n",
    "    'min_samples_split':[int(x) for x in np.linspace(2,15,14,endpoint=True,dtype = int)],\n",
    "    'n_estimators': [int(x) for x in np.linspace(50,1200,24,endpoint=True,dtype = int)]\n",
    "} #Grid Space for Hyper-parameters tuning\n",
    "\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfr, param_grid=param_grid, cv= 10,scoring='neg_mean_absolute_error',n_jobs = -1, verbose = 2)\n",
    "CV_rfc.fit(train_features,train_labels)# Grid Search with 10-fold Cross Validation for Hyper-parameter tuning\n",
    "####################Display Best set of Hyper-parameter \n",
    "print(CV_rfc.best_params_)\n",
    "##########Save the baseline random forest regression model with the results of hyper-parametrs tuning (CV_rfc.best_params_) \n",
    "BaseLineRFR=RandomForestRegressor(bootstrap=True,\n",
    "                                             max_depth=80,\n",
    "                                             min_samples_leaf=5,\n",
    "                                             min_samples_split=8,\n",
    "                                             n_estimators=100)\n",
    "import joblib\n",
    "\n",
    "joblib.dump(BaseLineRFR, \"./BaseLineModelrandom_forest_Regression.joblib\",compress=3)# Saving the Baseline model for future use\n",
    "\n",
    "\n",
    "###################################################### Step2: The performance Evaluation of baseline Random Forest Regression Model #################################\n",
    "\n",
    "BaseLineModel = joblib.load(\"./BaseLineModelrandom_forest_Regression.joblib\")# load the baseline model \n",
    "BaseLineModel.fit(train_features,train_labels)\n",
    "Bpredicted_labels_train_features=BaseLineModel.predict(train_features)#Predicting yield with training dataset\n",
    "Bpredicted_labels_test_features=BaseLineModel.predict(test_features)#Predicting yield with testing dataset\n",
    "\n",
    "############################################# Baseline Random Forest Regression Yield Model Performance Evaluation\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"Correlation Coefficient (r) of Baseline Random Forest Regression Model on Training Data: \",pearsonr(train_labels,Bpredicted_labels_train_features))\n",
    "print(\"Correlation Coefficient (r) of Baseline Random Forest Regression Model on Testing Data: \",pearsonr(test_labels,Bpredicted_labels_test_features))\n",
    "\n",
    "print(\"MAE of Baseline Random Forest Regression Model on Training Data: \",mean_absolute_error(train_labels,Bpredicted_labels_train_features))###t/ha\n",
    "print(\"MAE of Baseline Random Forest Regression Model on Testing Data: \",mean_absolute_error(test_labels,Bpredicted_labels_test_features))###t/ha\n",
    "\n",
    "print(\"RMSE of Baseline Random Forest Regression Model on Training Data: \",np.sqrt(mean_squared_error(train_labels,Bpredicted_labels_train_features)))###t/ha\n",
    "print(\"RMSE of Baseline Random Forest Regression Model on Testing Data: \",np.sqrt(mean_squared_error(test_labels,Bpredicted_labels_test_features)))###t/ha\n",
    "\n",
    "###################################################################################################################################################################\n",
    "##############################      Step 3: Opmtimised random forest regression with genetic algorithm (GA) based feature selection  ################\n",
    "\n",
    "from sklearn import linear_model\n",
    "from genetic_selection import GeneticSelectionCV\n",
    "\n",
    "                                        \n",
    "Featureselector = GeneticSelectionCV(BaseLineModel,\n",
    "                                  cv=5,\n",
    "                                  verbose=1,\n",
    "                                  scoring=\"neg_mean_absolute_error\", \n",
    "                                  max_features=20,\n",
    "                                  n_population=20,\n",
    "                                  crossover_proba=0.05,\n",
    "                                  mutation_proba=0.001,\n",
    "                                  n_generations=50,\n",
    "                                  crossover_independent_proba=0.5,\n",
    "                                  mutation_independent_proba=0.05,\n",
    "                                  tournament_size=3,\n",
    "                                  n_gen_no_change=10,\n",
    "                                  caching=True,\n",
    "                                  n_jobs=-1)#Feature Selection with Genetic Algorithm with Baseline Random ForestRegressor estimator with stopping criteria as NEGATIVE MSE\n",
    "FeatureselectorModel = Featureselector.fit(train_features,train_labels)#Fitting the training data into Opmtimised random forest regression with genetic algorithm (GA) based feature selection\n",
    "\n",
    "######################### Visualise the results of feature Selection ################\n",
    "featurename= list(features..columns.values)#List of Input Feature Names\n",
    "df = pd.DataFrame((featurename,Featureselector.support_,Featureselector.generation_scores_))### Feature Selection Result\n",
    "\n",
    "Transpose=df.T\n",
    "Transpose.columns =['Feature','Support','Score']\n",
    "Transpose.head()#Showing the Selected Features\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.factorplot(x= 'Feature', y= 'Support', data= Transpose, kind='bar', legend='True')\n",
    "plt.title('Feature Selection',fontsize=15)\n",
    "plt.show()#Ploting the selected features based on the Featureselector.support_\n",
    "\n",
    "\n",
    "##############################  Step 4: Performance evaluation of omtimised random forest regression with genetic algorithm (GA) based feature selection ############\n",
    "predicted_labels_train_features=FeatureselectorModel .predict(train_features)\n",
    "predicted_labels_test_features=FeatureselectorModel .predict(test_features)\n",
    "\n",
    "print(\"Correlation Coefficient (r) of Optimised Random Forest Regression Model on Training Data: \",pearsonr(train_labels,predicted_labels_train_features))\n",
    "print(\"Correlation Coefficient (r) of Optimised Random Forest Regression Model on Testing Data: \",pearsonr(test_labels,predicted_labels_test_features))\n",
    "\n",
    "print(\"MAE of Optimised Random Forest Regression Model on Training Data: \",mean_absolute_error(train_labels,predicted_labels_train_features))#t/ha\n",
    "print(\"MAE of Optimised Random Forest Regression Model on Testing Data: \",mean_absolute_error(test_labels,predicted_labels_test_features))#t/ha\n",
    "\n",
    "print(\"RMSE of Optimised Random Forest Regression Model on Training Data: \",np.sqrt(mean_squared_error(train_labels,predicted_labels_train_features)))#t/ha\n",
    "print(\"RMSE of Optimised Random Forest Regression Model on Testning Data: \",np.sqrt(mean_squared_error(test_labels,predicted_labels_test_features)))#t/ha\n",
    "\n",
    "################      Step 5: Visualisation of baseline and optimised model performance ############\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "######################################  Baseline Model##########################################\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.scatter(train_labels,Bpredicted_labels_train_features,s=10, c='b', marker=\"o\", label=\"Training Dataset = %.0f\"%(train_labels.shape))\n",
    "ax1.scatter(test_labels,Bpredicted_labels_test_features,s=10, c='r', marker=\"o\", label=\"Testing Dataset = %.0f\"%(test_labels.shape))\n",
    "plt.xlabel('Observed Yield (t/ha)',fontsize=15)\n",
    "plt.ylabel('Predicted Yield(t/ha)',fontsize=15)\n",
    "plt.title('Baseline Model',fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "############################ Optimised Model ###############\n",
    "ax1 = plt.subplot(1, 2, 2)\n",
    "ax1.scatter(train_labels,predicted_labels_train_features,s=10, c='b', marker=\"o\",label=\"Training Dataset = %.0f\"%(train_labels.shape))\n",
    "ax1.scatter(test_labels,predicted_labels_test_features,s=10, c='r', marker=\"o\", label=\"Testing Dataset = %.0f\"%(test_labels.shape))\n",
    "plt.xlabel('Observed Yield (t/ha)',fontsize=15)\n",
    "plt.ylabel('Predicted Yield(t/ha)',fontsize=15)\n",
    "plt.title('Optimised Model',fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
